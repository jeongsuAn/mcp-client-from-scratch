import json
import requests
import uuid
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

MCP_URL = "http://localhost:8000/mcp"

# https://modelcontextprotocol.io/specification/2025-06-18/basic/lifecycle
init_payload = {
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "roots": {
        "listChanged": True
      },
      "sampling": {},
      "elicitation": {}
    },
    "clientInfo": {
      "name": "ExampleClient",
      "title": "Example Client Display Name",
      "version": "1.0.0"
    }
  }
}

custom_headers = {
    "Accept": "application/json, text/event-stream"
}

resp = requests.post(MCP_URL, json=init_payload, headers=custom_headers)

print(resp.content.decode())
# --- ì‘ë‹µ ì •ë³´ ---
# Status Code: 200
# Response Headers: 
#   {'date': 'Wed, 12 Nov 2025 13:15:07 GMT', 
#    'server': 'uvicorn', 
#    'cache-control': 'no-cache, no-transform',
#    'connection': 'keep-alive', 
#    'content-type': 'text/event-stream', 
#    'mcp-session-id': '2d27d9b84afc45cd983e256b2772dab0', 
#    'x-accel-buffering': 'no', 
#    'Transfer-Encoding': 'chunked'
#   }
# Response Text : event: message
# data: 
#   {"jsonrpc":"2.0",
#    "id":1,
#    "result":{
#       "protocolVersion":"2025-06-18",
#       "capabilities":{
#           "experimental":{},
#           "prompts":{"listChanged":true},
#           "resources":{"subscribe":false,"listChanged":true},
#           "tools":{"listChanged":true}
#         },
#         "serverInfo":{
#           "name":"jeongsu_demo",
#           "version":"2.13.0.2"
#         }
#       }
#     }
#   }


# ì •ìƒì ì¸ ì‘ì—…ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆë‹¤ëŠ” ì•Œë¦¼ì„ ë³´ë‚´ì•¼í•œë‹¤. 
session_id = resp.headers.get("mcp-session-id")
custom_headers["MCP-Session-ID"] = session_id
ready_payload = {
  "jsonrpc": "2.0",
  "method": "notifications/initialized"
}
resp = requests.post(MCP_URL, json=ready_payload, headers=custom_headers) # custom_headersëŠ” í•­ìƒ ìˆì–´ì•¼í•œë‹¤.
print("\nready response:")
print(resp.status_code) 
print(resp.headers)  
print(resp.content.decode()) 

# ready response:
# 202
# {'date': 'Wed, 12 Nov 2025 13:38:31 GMT', 'server': 'uvicorn', 'content-type': 'application/json', 'mcp-session-id': '9def9a00b1bf4f3bad7a43b070bb3f5b', 'content-length': '0'}


# íˆ´ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ 
# https://modelcontextprotocol.io/specification/2025-06-18/server/tools
tools_payload = {
  "jsonrpc": "2.0",
  "id": 1,
  "method": "tools/list",
  "params": {
    "cursor": "optional-cursor-value"
  }
}
resp = requests.post(MCP_URL, json=tools_payload, headers=custom_headers)
print("\ntools response:")
print(resp.status_code)  # 200
print(resp.headers)
print(resp.content.decode())  # íˆ´ ë¦¬ìŠ¤íŠ¸ ì •ë³´

raw_text = resp.content.decode()
print(raw_text)  # ì‘ë‹µ ì›ë³¸ (event: message\ndata: ...)

# --- ğŸ’¡ ì—¬ê¸°ë¶€í„° ìˆ˜ì •ë¨ ğŸ’¡ ---

json_string = None
# 1. ì‘ë‹µì„ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
for line in raw_text.splitlines():
    # 2. "data: "ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ì„ ì°¾ìŒ
    if line.startswith("data: "):
        # 3. "data: " ë¶€ë¶„ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€(ìˆœìˆ˜ JSON ë¬¸ìì—´)ë¥¼ ì¶”ì¶œ
        json_string = line[len("data: "):].strip()
        break

tools_list = []
# 4. JSON ë¬¸ìì—´ì„ ì„±ê³µì ìœ¼ë¡œ ì°¾ì€ ê²½ìš°ì—ë§Œ íŒŒì‹±
if json_string:
    try:
        # 5. 'json()'ì´ ì•„ë‹Œ 'json.loads()'ë¡œ íŒŒì‹±
        data = json.loads(json_string)
        tools_list = data.get("result", {}).get("tools", [])
    except json.JSONDecodeError as e:
        print(f"JSON íŒŒì‹± ì—ëŸ¬: {e}")
        print(f"íŒŒì‹± ì‹œë„í•œ ë¬¸ìì—´: {json_string}")
else:
    print("ì‘ë‹µì—ì„œ 'data:' ë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# --- ğŸ’¡ ìˆ˜ì • ë ğŸ’¡ ---

print("\n--- ì¶”ì¶œëœ ë„êµ¬ ëª©ë¡ ---")
if tools_list:
    for tool in tools_list:
        print(tool.get("name"), " : ", tool.get("description"))
else:
    print("ë„êµ¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# 200
# {'date': 'Wed, 12 Nov 2025 14:02:06 GMT', 'server': 'uvicorn', 'cache-control': 'no-cache, no-transform', 'connection': 'keep-alive', 'content-type': 'text/event-stream', 'mcp-session-id': '1c5611eabf06439a9c178b4c7ba13a4f', 'x-accel-buffering': 'no', 'Transfer-Encoding': 'chunked'}
# event: message
# data: {"jsonrpc":"2.0","id":1,"result":{"tools":[{"name":"current_time","inputSchema":{"properties":{},"type":"object"},"outputSchema":{"properties":{"result":{"type":"string"}},"required":["result"],"type":"object","x-fastmcp-wrap-result":true},"_meta":{"_fastmcp":{"tags":[]}}}]}}


client = OpenAI()

def convert_mcp_tool_to_openai(mcp_tool: dict) -> dict:
    """MCP ë„êµ¬ ëª…ì„¸ë¥¼ OpenAI 'function' ë„êµ¬ ëª…ì„¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return {
        "type": "function",
        "function": {
            "name": mcp_tool.get("name"),
            "description": mcp_tool.get("description", ""), # descriptionì´ ì—†ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´
            "parameters": mcp_tool.get("inputSchema", {"type": "object", "properties": {}})
        }
    }

# 2. MCP ì„œë²„ì— ì‹¤ì œ 'tools/call'ì„ ìš”ì²­í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
# https://modelcontextprotocol.io/specification/2025-06-18/server/tools
def call_mcp_tool(session_headers: dict, tool_name: str, tool_args: dict) -> any:
    """OpenAIê°€ ìš”ì²­í•œ ë„êµ¬ë¥¼ ì‹¤ì œ MCP ì„œë²„ì— 'tools/call'ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print(f"--- MCP ì„œë²„ì— 'tools/call' ìš”ì²­: {tool_name}({tool_args}) ---")
    
    mcp_tool_call_id = str(uuid.uuid4())
    payload = {
        "jsonrpc": "2.0",
        "id": mcp_tool_call_id, # ì´ ìš”ì²­ì„ ìœ„í•œ ìƒˆ ID
        "method": "tools/call",
        "params": {
            "name": tool_name,
            "arguments": tool_args,
        }
    }
    
    try:
        resp = requests.post(MCP_URL, json=payload, headers=session_headers)
        resp.raise_for_status() # HTTP ì—ëŸ¬ ì²´í¬

        raw_text = resp.content.decode()
        print(f"MCP 'tools/call' ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {resp.status_code}")
        print(f"MCP 'tools/call' ì‘ë‹µ ì›ë³¸:\n{raw_text}")
        
        
        for line in raw_text.splitlines():
            if line.startswith("data: "):
                json_string = line[len("data: "):].strip()
                
                # data: ë¼ì¸ ë’¤ì— ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
                if not json_string:
                    continue
                    
                try:
                    data = json.loads(json_string)
                except json.JSONDecodeError as e:
                    print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}, ë¼ì¸: {json_string}")
                    continue # ë‹¤ìŒ ë¼ì¸ ì‹œë„

                # 1. ìµœìš°ì„ : MCPê°€ 'error'ë¥¼ ë°˜í™˜í–ˆëŠ”ì§€ í™•ì¸
                if "error" in data:
                    print(f"MCP ë„êµ¬ ì‹¤í–‰ ì—ëŸ¬: {data['error']}")
                    return f"Error: {data['error'].get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"

                # 2. 'result' í‚¤ê°€ ìˆê³ , ê·¸ê²ƒì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                if "result" in data and isinstance(data.get("result"), dict):
                    mcp_result_data = data["result"]
                    
                    # 3. ìƒˆë¡œìš´ í‘œì¤€ ê²½ë¡œ (structuredContent.result) í™•ì¸
                    if ("structuredContent" in mcp_result_data and 
                        isinstance(mcp_result_data.get("structuredContent"), dict) and 
                        "result" in mcp_result_data["structuredContent"]):
                        
                        mcp_result = mcp_result_data["structuredContent"]["result"]
                        print(f"MCP ë„êµ¬ ì‹¤í–‰ ê²°ê³¼: {mcp_result}")
                        return mcp_result # ì„±ê³µ! ê²°ê³¼ ë°˜í™˜

                    # 4. ê¸°ì¡´ ê²½ë¡œ (result.result)ë„ í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ í™•ì¸
                    if "result" in mcp_result_data:
                        mcp_result = mcp_result_data["result"]
                        print(f"MCP ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ (ëŒ€ì²´ ê²½ë¡œ): {mcp_result}")
                        return mcp_result # ì„±ê³µ! ê²°ê³¼ ë°˜í™˜
                
                # 1, 2, 3, 4 ëª¨ë‘ í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ ìœ íš¨í•œ ì‘ë‹µ êµ¬ì¡°ê°€ ì•„ë‹˜
                # (ë‹¤ìŒ data: ë¼ì¸ì„ ìœ„í•´ ë£¨í”„ ê³„ì†)

        # for ë£¨í”„ë¥¼ ëª¨ë‘ ëŒì•˜ëŠ”ë° ìœ íš¨í•œ dataë¥¼ ëª»ì°¾ìŒ
        return "Error: MCPì—ì„œ ìœ íš¨í•œ 'data:' ì‘ë‹µ ë˜ëŠ” ê²°ê³¼ í•„ë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    except requests.RequestException as e:
        print(f"MCP 'tools/call' ìš”ì²­ ì‹¤íŒ¨: {e}")
        return f"Error: {e}"
    except json.JSONDecodeError as e:
        print(f"MCP 'tools/call' ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return "Error: Failed to parse MCP response"


# --- ğŸ’¡ ë©”ì¸ ë¡œì§ ğŸ’¡ ---

print("\n--- OpenAI Tool Call ì‹œì‘ ---")

# 3. MCP ë„êµ¬ ëª©ë¡ì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
openai_tools = [convert_mcp_tool_to_openai(tool) for tool in tools_list]
print(f"OpenAIì— {len(openai_tools)}ê°œì˜ ë„êµ¬ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤: {[t['function']['name'] for t in openai_tools]}")

# 4. ì‚¬ìš©ì ì§ˆë¬¸ ì •ì˜ (ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìœ ë„)
user_prompt = "ì„œìš¸ ë‚ ì”¨ ì–´ë–»ê²Œ ë¼?"
print(f"\nì‚¬ìš©ì ì§ˆë¬¸: {user_prompt}")

messages = [
    {"role": "user", "content": user_prompt}
]

# 5. ì²« ë²ˆì§¸ OpenAI í˜¸ì¶œ (ë„êµ¬ ëª©ë¡ê³¼ í•¨ê»˜)
try:
    response = client.chat.completions.create(
        model="gpt-5-nano", 
        messages=messages,
        tools=openai_tools,
        tool_choice="auto"  # OpenAIê°€ ë„êµ¬ ì‚¬ìš©ì„ ê²°ì •
    )

    response_message = response.choices[0].message
    tool_calls = response_message.tool_calls

    # 6. OpenAIê°€ ë„êµ¬ ì‚¬ìš©ì„ ìš”ì²­í–ˆëŠ”ì§€ í™•ì¸
    if tool_calls:
        print("\nOpenAIê°€ ë„êµ¬ í˜¸ì¶œì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.")
        print("tool_calls : ", tool_calls)
        # ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µ(ë„êµ¬ í˜¸ì¶œ ìš”ì²­)ì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        messages.append(response_message)

        tool_outputs = [] # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

        # 7. ìš”ì²­ëœ ëª¨ë“  ë„êµ¬ ì‹¤í–‰
        for tool_call in tool_calls:
            
            tool_name = tool_call.function.name
            # tool_call.function.argumentsëŠ” JSON *ë¬¸ìì—´*ì´ë¯€ë¡œ íŒŒì‹± í•„ìš”
            print("ë„êµ¬ í˜¸ì¶œ ìš”ì²­:", tool_name, tool_call.function.arguments)
            tool_args = json.loads(tool_call.function.arguments) 
            oa_tool_call_id = tool_call.id # OpenAIê°€ ë¶€ì—¬í•œ ì´ í˜¸ì¶œì˜ ê³ ìœ  ID
 
            # ìœ„ì—ì„œ ë§Œë“  í—¬í¼ í•¨ìˆ˜ë¡œ MCP ì„œë²„ì— ì‹¤ì œ ë„êµ¬ ì‹¤í–‰ ìš”ì²­
            mcp_result = call_mcp_tool(
                session_headers=custom_headers,
                tool_name=tool_name,
                tool_args=tool_args
            )

            # 8. ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ OpenAIê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì¶”ê°€
            tool_outputs.append({
                "tool_call_id": oa_tool_call_id,
                "role": "tool",
                "content": json.dumps({"result": mcp_result}) # ê²°ê³¼ë¥¼ JSON ë¬¸ìì—´ë¡œ ì „ë‹¬
            })

        # 9. ë‘ ë²ˆì§¸ OpenAI í˜¸ì¶œ (ë„êµ¬ ê²°ê³¼ì™€ í•¨ê»˜)
        print("ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ OpenAIì— ì „ë‹¬í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        
        # ëŒ€í™” ê¸°ë¡ì— ë„êµ¬ ê²°ê³¼(ë“¤) ì¶”ê°€
        messages.extend(tool_outputs)
        print("ìµœì¢… ë©”ì‹œì§€ ê¸°ë¡:", messages)

        final_response = client.chat.completions.create(
            model="gpt-5-nano",
            messages=messages # ì „ì²´ ëŒ€í™” ê¸°ë¡ (User -> Assistant(tool) -> Tool(result))
        )

        final_answer = final_response.choices[0].message.content
        print("\n--- ğŸ¤– ìµœì¢… ë‹µë³€ ---")
        print(final_answer)

    else:
        # OpenAIê°€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë°”ë¡œ ë‹µë³€í•œ ê²½ìš°
        print("\n--- ğŸ¤– ìµœì¢… ë‹µë³€ (ë„êµ¬ ë¯¸ì‚¬ìš©) ---")
        print(response_message.content)

except Exception as e:
    print(f"OpenAI API ì˜¤ë¥˜ ë°œìƒ: {e}")